{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaniamv/final-project-edit/blob/main/Streaming_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aplication in Real Time to Read Carris API - group 1\n"
      ],
      "metadata": {
        "id": "fh0TmAWvVoCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Authentication to gcloud"
      ],
      "metadata": {
        "id": "B9KbgW3BVxtL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0-WlCSjaVZiz",
        "outputId": "384376c9-a76f-448c-8be4-b0ce08b43b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=bkX1TEEMxAk1ufk0TNMXLpxTBzMkvh&prompt=consent&token_usage=remote&access_type=offline&code_challenge=Th6LJRA5MLhPhEsNsIjFgdVCpHXQVdLv9MzdbZcQTCU&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0ASVgi3JmNVNq5TW3c2Urfe3hULxlpQj8BuzpruL0OpAeiyGFxDPccJhNjLv--IhlCKhZxA\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ],
      "source": [
        "# autentication to gcloud with login\n",
        "\n",
        "!gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download connector and save it local\n",
        "\n",
        "!wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar -P /usr/local/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IuHDKJ_SV3Qq",
        "outputId": "11098b2f-7bac-4baa-8300-12fa76746665"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-22 13:58:52--  https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33831577 (32M) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar’\n",
            "\n",
            "gcs-connector-hadoo 100%[===================>]  32.26M   172MB/s    in 0.2s    \n",
            "\n",
            "2025-01-22 13:58:52 (172 MB/s) - ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar’ saved [33831577/33831577]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('GCS_Spark') \\\n",
        "    .config('spark.jars', '/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar') \\\n",
        "    .config('spark.hadoop.fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# save credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/content/.config/application_default_credentials.json'\n",
        "\n",
        "# Config PySpark to access the GCS\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", '/content/.config/application_default_credentials.json')"
      ],
      "metadata": {
        "id": "2XXRK45iV5s_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Read Stream"
      ],
      "metadata": {
        "id": "4bbr84t-WILm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# create schema\n",
        "vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                             StructField('block_id', StringType(), True),\n",
        "                             StructField('current_status', StringType(), True),\n",
        "                             StructField('id', StringType(), True),\n",
        "                             StructField('lat', FloatType(), True),\n",
        "                             StructField('line_id', StringType(), True),\n",
        "                             StructField('lon', FloatType(), True),\n",
        "                             StructField('pattern_id', StringType(), True),\n",
        "                             StructField('route_id', StringType(), True),\n",
        "                             StructField('schedule_relationship', StringType(), True),\n",
        "                             StructField('shift_id', StringType(), True),\n",
        "                             StructField('speed', FloatType(), True),\n",
        "                             StructField('stop_id', StringType(), True),\n",
        "                             StructField('timestamp', TimestampType(), True),\n",
        "                             StructField('trip_id', StringType(), True)])\n",
        "\n",
        "\n",
        "#readStreaming\n",
        "stream = spark.readStream.format(\"json\").schema(vehicle_schema).load(\"gs://edit-de-project-streaming-data/carris-vehicles\")"
      ],
      "metadata": {
        "id": "d6nbxv9sWL-K"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Read from API endpoint stops"
      ],
      "metadata": {
        "id": "4lMfv4ejWVY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_stops = spark.read.option(\"header\", \"true\").csv('gs://edit-data-eng-project-group1/LandingZone/GTFS/stops.txt')\n",
        "df_stops = df_stops.select('stop_id','stop_lat','stop_lon')\n",
        "df_stops = df_stops.withColumn(\"stop_lat\", df_stops[\"stop_lat\"].cast(\"float\"))\n",
        "df_stops = df_stops.withColumn(\"stop_lon\", df_stops[\"stop_lon\"].cast(\"float\"))"
      ],
      "metadata": {
        "id": "vELIsRG_WRTh"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select columns\n",
        "transform = stream.select('id', 'speed', 'timestamp','line_id','route_id','stop_id','lat', 'lon')\n",
        "# join tables\n",
        "transform = transform.join(df_stops, on='stop_id', how='left')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fIS5epvuhXMM"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col, lag,coalesce, current_timestamp, window\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# watermark is necessarary because of the aggregation\n",
        "transformed = transform.withWatermark(\"timestamp\", \"60 seconds\")\n",
        "\n",
        "windowed_transform = transformed.groupBy(\"id\", \"stop_id\", F.window(\"timestamp\", \"2 minutes\")).agg(\n",
        "    F.first(col(\"lat\")).alias(\"previous_lat\"),\n",
        "    F.first(col(\"lon\")).alias(\"previous_lon\"),\n",
        "    F.last(col(\"lat\")).alias(\"lat\"),\n",
        "    F.last(col(\"lon\")).alias(\"lon\"),\n",
        "    F.last(col(\"stop_lat\")).alias(\"stop_lat\"),\n",
        "    F.last(col(\"stop_lon\")).alias(\"stop_lon\")\n",
        "    )"
      ],
      "metadata": {
        "id": "G7jkm0qSpW7E"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "\n",
        "    if any(x is None for x in [lat1, lon1, lat2, lon2]):\n",
        "        return 0.0\n",
        "    R = 6371  # Earth's radius in kilometers\n",
        "\n",
        "    # Convert latitude and longitude to radians\n",
        "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "    # Calculate differences\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "\n",
        "    # Apply Haversine formula\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.asin(math.sqrt(a))\n",
        "\n",
        "    # Calculate distance\n",
        "    distance = R * c\n",
        "\n",
        "    return distance\n",
        "\n",
        "# Register the UDF\n",
        "distance_udf = udf(haversine_distance, FloatType())\n",
        "\n",
        "windowed_transform = windowed_transform.withColumn(\"distance\", distance_udf(windowed_transform[\"previous_lat\"],windowed_transform[\"previous_lon\"],windowed_transform[\"lat\"],windowed_transform[\"lon\"]))\n",
        "windowed_transform = windowed_transform.withColumn(\"distance_to_stop\", distance_udf(windowed_transform[\"lat\"],windowed_transform[\"lon\"],windowed_transform[\"stop_lat\"],windowed_transform[\"stop_lon\"]))\n"
      ],
      "metadata": {
        "id": "GbBvKd5PeCtA"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg = windowed_transform.withColumn('speed', col('distance')/(2/60))\n",
        "\n",
        "agg = agg.filter(agg.distance_to_stop.isNotNull() & (agg.distance_to_stop > 0) & (agg.speed.isNotNull()) & (agg.speed > 0)) \\\n",
        "         .withColumn('time_to_stop', (col('distance_to_stop')/col('speed') * 3600))\n",
        "\n",
        "agg = agg.withColumn(\n",
        "    'time_to_stop',\n",
        "    F.from_unixtime(\n",
        "        F.unix_timestamp(F.lit('00:00:00'), 'HH:mm:ss') + col('time_to_stop'),\n",
        "        'HH:mm:ss'\n",
        "    ))"
      ],
      "metadata": {
        "id": "SGF0LtzIgx7g"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write Stream"
      ],
      "metadata": {
        "id": "_J96nYkbXB06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select folder\n",
        "folder = 'stream/vehicles2'\n",
        "gc_folder = 'gs://edit-de-project-streaming-data/datalake/stream/vehicles'\n",
        "\n",
        "\n",
        "# Output function for each windowed batch\n",
        "def insert_windowed_vehicles(df, batch_id):\n",
        "    print(f\"Batch ID: {batch_id}\")\n",
        "    df.write.format(\"parquet\").mode(\"append\").save(f\"{folder}\")\n",
        "\n",
        "\n",
        "# Write the streaming query with watermark and window\n",
        "windowed_query = (agg\n",
        "                  .writeStream\n",
        "                  .outputMode(\"append\")\n",
        "                  .foreachBatch(insert_windowed_vehicles)\n",
        "                  .option('checkpointLocation', f'{folder}/checkpoint')\n",
        "                  .trigger(processingTime='10 seconds')\n",
        "                  .start()\n",
        ")\n",
        "\n",
        "windowed_query.awaitTermination(30)"
      ],
      "metadata": {
        "id": "eaDegbFrhpqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b273a2dc-adca-4ed5-e9ec-4481f4e2e84d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.isActive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SPUcGoQiipf",
        "outputId": "745e8ccc-9c8a-4f2a-df9b-066046c1b581"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C04cg6jJxTfM",
        "outputId": "9695a28d-a997-4a9f-b437-3b205c99fbdb"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.recentProgress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n90r0H5bhS7K",
        "outputId": "a1b37e4a-1ef6-4d59-b699-f859ebcf9966"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#windowed_query.stop()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xKTj7O-Qiqxv"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Transformations"
      ],
      "metadata": {
        "id": "bmsfBvacWi5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WdiewdrydX7p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMgMV1oVWiCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.recentProgress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve7gJIYxw2hY",
        "outputId": "6e350d5a-fee3-4be9-fd3c-0841c5d045c2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the Parquet files\n",
        "parquet_path = \"stream/vehicles2\"\n",
        "\n",
        "# Read the Parquet files into a DataFrame\n",
        "parquet_df = spark.read.parquet(parquet_path)\n",
        "\n",
        "# Show the first few rows\n",
        "parquet_df.show(truncate=False)\n",
        "\n",
        "# Print the schema to understand the data structure\n",
        "parquet_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MV5AxogihP9",
        "outputId": "772de29e-8c4f-4c29-d5af-2f3618333de1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------+------------+---+---+--------+--------+--------+----------------+-----+------------+\n",
            "|id |stop_id|window|previous_lat|previous_lon|lat|lon|stop_lat|stop_lon|distance|distance_to_stop|speed|time_to_stop|\n",
            "+---+-------+------+------------+------------+---+---+--------+--------+--------+----------------+-----+------------+\n",
            "+---+-------+------+------------+------------+---+---+--------+--------+--------+----------------+-----+------------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- stop_id: string (nullable = true)\n",
            " |-- window: struct (nullable = true)\n",
            " |    |-- start: timestamp (nullable = true)\n",
            " |    |-- end: timestamp (nullable = true)\n",
            " |-- previous_lat: float (nullable = true)\n",
            " |-- previous_lon: float (nullable = true)\n",
            " |-- lat: float (nullable = true)\n",
            " |-- lon: float (nullable = true)\n",
            " |-- stop_lat: float (nullable = true)\n",
            " |-- stop_lon: float (nullable = true)\n",
            " |-- distance: float (nullable = true)\n",
            " |-- distance_to_stop: float (nullable = true)\n",
            " |-- speed: double (nullable = true)\n",
            " |-- time_to_stop: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GoIrIPqBicIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}