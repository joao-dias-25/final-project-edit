{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaniamv/final-project-edit/blob/main/Streaming_RT_ETL_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh0TmAWvVoCq"
      },
      "source": [
        "# Aplication in Real Time to Read Carris data - group 1 - ETL approach\n",
        "\n",
        "This notebook documents the steps to implement a data pipeline leveraging Google Cloud Platform (GCP), following an ETL (Extract, Transform, Load) approach. The pipeline processes data in three stages:\n",
        "\n",
        "* Streaming Ingestion and Transformation (Extract and Transform): Data is ingested in real-time from a bucket that gets the vehicles endpoint of Carris API. During ingestion, transformations are applied directly to the data stream, such as cleaning, enrichment, and standardization, ensuring that only processed and structured data flows through the pipeline.\n",
        "\n",
        "* Loading Transformed Data: The pre-processed data is then stored in a silver layer bucket on GCP. This layer serves as a structured repository, optimized for downstream analytical queries and consumption.\n",
        "\n",
        "By prioritizing the ETL approach, this pipeline ensures that the data is transformed as it is ingested, minimizing the need for post-processing and enabling faster delivery of structured and actionable insights.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9KbgW3BVxtL"
      },
      "source": [
        "1. Authentication to Google Cloud Platform (GCP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0-WlCSjaVZiz",
        "outputId": "91dd6317-a736-46c6-d1e0-72391cf74201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The environment variable [GOOGLE_APPLICATION_CREDENTIALS] is set to:\n",
            "  [/content/.config/application_default_credentials.json]\n",
            "Credentials will still be generated to the default location:\n",
            "  [/content/.config/application_default_credentials.json]\n",
            "To use these credentials, unset this environment variable before\n",
            "running your application.\n",
            "\n",
            "Do you want to continue (Y/n)?  n\n",
            "\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.auth.application-default.login) Aborted by user.\n"
          ]
        }
      ],
      "source": [
        "# autentication to gcloud with login\n",
        "\n",
        "!gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IuHDKJ_SV3Qq",
        "outputId": "21b95889-61b9-4742-9ff4-3b852a9b91d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-23 19:21:58--  https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33831577 (32M) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar’\n",
            "\n",
            "gcs-connector-hadoo 100%[===================>]  32.26M   166MB/s    in 0.2s    \n",
            "\n",
            "2025-01-23 19:21:58 (166 MB/s) - ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar’ saved [33831577/33831577]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download connector and save it local\n",
        "\n",
        "!wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar -P /usr/local/lib/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "2. Initialize SparkSession and set up the access to GCS\n"
      ],
      "metadata": {
        "id": "XyXhm9dMmSQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2XXRK45iV5s_"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('GCS_Spark') \\\n",
        "    .config('spark.jars', '/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar') \\\n",
        "    .config('spark.hadoop.fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# save credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/content/.config/application_default_credentials.json'\n",
        "\n",
        "# Config PySpark to access the GCS\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", '/content/.config/application_default_credentials.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bbr84t-WILm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "3. Set up the source schema and initialize the readStream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "d6nbxv9sWL-K"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# create schema\n",
        "vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                             StructField('block_id', StringType(), True),\n",
        "                             StructField('current_status', StringType(), True),\n",
        "                             StructField('id', StringType(), True),\n",
        "                             StructField('lat', FloatType(), True),\n",
        "                             StructField('line_id', StringType(), True),\n",
        "                             StructField('lon', FloatType(), True),\n",
        "                             StructField('pattern_id', StringType(), True),\n",
        "                             StructField('route_id', StringType(), True),\n",
        "                             StructField('schedule_relationship', StringType(), True),\n",
        "                             StructField('shift_id', StringType(), True),\n",
        "                             StructField('speed', FloatType(), True),\n",
        "                             StructField('stop_id', StringType(), True),\n",
        "                             StructField('timestamp', TimestampType(), True),\n",
        "                             StructField('trip_id', StringType(), True)])\n",
        "\n",
        "\n",
        "#readStreaming\n",
        "stream = spark.readStream.format(\"json\").schema(vehicle_schema).load(\"gs://edit-de-project-streaming-data/carris-vehicles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lMfv4ejWVY_"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "4. Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vELIsRG_WRTh"
      },
      "outputs": [],
      "source": [
        "df_stops = spark.read.option(\"header\", \"true\").csv('gs://edit-data-eng-project-group1/LandingZone/GTFS/stops.txt')\n",
        "df_stops = df_stops.select('stop_id','stop_lat','stop_lon')\n",
        "df_stops = df_stops.withColumn(\"stop_lat\", df_stops[\"stop_lat\"].cast(\"float\"))\n",
        "df_stops = df_stops.withColumn(\"stop_lon\", df_stops[\"stop_lon\"].cast(\"float\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": true,
        "id": "fIS5epvuhXMM"
      },
      "outputs": [],
      "source": [
        "#select columns\n",
        "transform = stream.select('id', 'timestamp','stop_id','lat', 'lon')\n",
        "# join tables\n",
        "transform = transform.join(df_stops, on='stop_id', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rMgMV1oVWiCy"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lag , col, coalesce, window\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql import functions as F\n",
        "import math\n",
        "\n",
        "# Define watermarking and window duration\n",
        "watermark_duration = \"120 seconds\"\n",
        "window_duration = \"2 minutes\"\n",
        "\n",
        "# Using aggregate functions to get \"last known\" data within the window\n",
        "windowed_transform = transform \\\n",
        "  .withWatermark(\"timestamp\", watermark_duration) \\\n",
        "  .groupBy(F.window(\"timestamp\", window_duration),\"id\") \\\n",
        "  .agg(\n",
        "      F.max(col(\"timestamp\")).alias(\"max_ts\"),\n",
        "      F.first(\"lat\", True).alias(\"previous_lat\"),\n",
        "      F.first(\"lon\", True).alias(\"previous_lon\"),\n",
        "      F.last(\"lat\", True).alias(\"lat\"),\n",
        "      F.last(\"lon\", True).alias(\"lon\"),\n",
        "      F.last(\"stop_lat\", True).alias(\"stop_lat\"),\n",
        "      F.last(\"stop_lon\", True).alias(\"stop_lon\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "\n",
        "    if any(x is None for x in [lat1, lon1, lat2, lon2]):\n",
        "        return 0.0\n",
        "    R = 6371  # Earth's radius in kilometers\n",
        "\n",
        "    # Convert latitude and longitude to radians\n",
        "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "    # Calculate differences\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "\n",
        "    # Apply Haversine formula\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.asin(math.sqrt(a))\n",
        "\n",
        "    # Calculate distance\n",
        "    distance = R * c\n",
        "\n",
        "    return distance\n",
        "\n",
        "distance_udf = udf(haversine_distance, FloatType())\n",
        "\n",
        "windowed_transform = windowed_transform.withColumn(\"distance_traveled\", distance_udf(windowed_transform[\"previous_lat\"],windowed_transform[\"previous_lon\"],windowed_transform[\"lat\"],windowed_transform[\"lon\"]))\n",
        "windowed_transform = windowed_transform.withColumn(\"distance_to_stop\", distance_udf(windowed_transform[\"lat\"],windowed_transform[\"lon\"],windowed_transform[\"stop_lat\"],windowed_transform[\"stop_lon\"]))"
      ],
      "metadata": {
        "id": "LCEv2498urqk"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg = windowed_transform.withColumn('speed', col('distance_traveled')/(2/60))\n",
        "\n",
        "agg = agg.filter(agg.distance_to_stop.isNotNull() & (agg.distance_to_stop > 0) & (agg.speed.isNotNull()) & (agg.speed > 0)) \\\n",
        "         .withColumn('time_to_stop', (col('distance_to_stop')/col('speed') * 3600))\n",
        "\n",
        "agg = agg.withColumn(\n",
        "    'time_to_stop',\n",
        "    F.from_unixtime(\n",
        "        F.unix_timestamp(F.lit('00:00:00'), 'HH:mm:ss') + col('time_to_stop'),\n",
        "        'HH:mm:ss'\n",
        "    ))"
      ],
      "metadata": {
        "id": "-uoHHtL2pzVr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J96nYkbXB06"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "5. Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaDegbFrhpqJ"
      },
      "outputs": [],
      "source": [
        "# Output function for each windowed batch\n",
        "def insert_windowed_vehicles(df, batch_id):\n",
        "    print(f\"Batch ID: {batch_id} - Starting\")\n",
        "    try:\n",
        "      df.write.format(\"parquet\").mode(\"append\").save(\"gs://edit-data-eng-project-group1/datalake/stream/ETL/\")\n",
        "      print(f\"Batch ID: {batch_id} - Finished\")\n",
        "    except Exception as e:\n",
        "      print(f\"Batch ID: {batch_id} - Error: {e}\")\n",
        "\n",
        "# Write the streaming query with watermark and window\n",
        "windowed_query = (agg\n",
        "                  .writeStream\n",
        "                  .outputMode(\"update\")\n",
        "                  .foreachBatch(insert_windowed_vehicles)\n",
        "                  .option('checkpointLocation', 'gs://edit-data-eng-project-group1/datalake/stream/ETL/checkpoint')\n",
        "                  .trigger(processingTime='31 seconds')\n",
        "                  .start()\n",
        ")\n",
        "\n",
        "windowed_query.awaitTermination(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SPUcGoQiipf"
      },
      "outputs": [],
      "source": [
        "windowed_query.isActive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pmIWYYFWolS",
        "outputId": "9c83844c-e4c3-4c85-f7c7-02246ec7e407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(4) HashAggregate(keys=[window#5542, id#5560], functions=[max(timestamp#5570-T120000ms), last(lat#5561, true), last(lon#5563, true), first(lat#5561, true), first(lon#5563, true)])\n",
            "+- StateStoreSave [window#5542, id#5560], state info [ checkpoint = gs://edit-data-eng-project-group1/datalake/stream/windowed_vehicles_1/checkpoint/state, runId = d6dca9f5-8bbe-43f0-b9bb-4d21c90f5506, opId = 0, ver = 0, numPartitions = 200], Update, 0, 0, 2\n",
            "   +- *(3) HashAggregate(keys=[window#5542, id#5560], functions=[merge_max(timestamp#5570-T120000ms), merge_last(lat#5561, true), merge_last(lon#5563, true), merge_first(lat#5561, true), merge_first(lon#5563, true)])\n",
            "      +- StateStoreRestore [window#5542, id#5560], state info [ checkpoint = gs://edit-data-eng-project-group1/datalake/stream/windowed_vehicles_1/checkpoint/state, runId = d6dca9f5-8bbe-43f0-b9bb-4d21c90f5506, opId = 0, ver = 0, numPartitions = 200], 2\n",
            "         +- *(2) HashAggregate(keys=[window#5542, id#5560], functions=[merge_max(timestamp#5570-T120000ms), merge_last(lat#5561, true), merge_last(lon#5563, true), merge_first(lat#5561, true), merge_first(lon#5563, true)])\n",
            "            +- Exchange hashpartitioning(window#5542, id#5560, 200), ENSURE_REQUIREMENTS, [plan_id=6707]\n",
            "               +- *(1) HashAggregate(keys=[window#5542, id#5560], functions=[partial_max(timestamp#5570-T120000ms), partial_last(lat#5561, true), partial_last(lon#5563, true), partial_first(lat#5561, true), partial_first(lon#5563, true)])\n",
            "                  +- *(1) Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#5570-T120000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#5570-T120000ms, TimestampType, LongType) - 0) % 100000000) < 0) THEN (((precisetimestampconversion(timestamp#5570-T120000ms, TimestampType, LongType) - 0) % 100000000) + 100000000) ELSE ((precisetimestampconversion(timestamp#5570-T120000ms, TimestampType, LongType) - 0) % 100000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#5570-T120000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#5570-T120000ms, TimestampType, LongType) - 0) % 100000000) < 0) THEN (((precisetimestampconversion(timestamp#5570-T120000ms, TimestampType, LongType) - 0) % 100000000) + 100000000) ELSE ((precisetimestampconversion(timestamp#5570-T120000ms, TimestampType, LongType) - 0) % 100000000) END) - 0) + 100000000), LongType, TimestampType))) AS window#5542, id#5560, lat#5561, lon#5563, timestamp#5570-T120000ms]\n",
            "                     +- *(1) Filter isnotnull(timestamp#5570-T120000ms)\n",
            "                        +- EventTimeWatermark timestamp#5570: timestamp, 2 minutes\n",
            "                           +- FileScan json [id#5560,lat#5561,lon#5563,timestamp#5570] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(6064 paths)[gs://edit-de-project-streaming-data/carris-vehicles/vehicles_173729..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,lat:float,lon:float,timestamp:timestamp>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "windowed_query.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C04cg6jJxTfM",
        "outputId": "7edf8b40-9813-414b-ba01-6e6f8a916520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "windowed_query.status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve7gJIYxw2hY",
        "outputId": "4fddb979-f91d-46a9-cf49-32115f5b107a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "windowed_query.recentProgress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "id": "xKTj7O-Qiqxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0a4c28-fc46-472d-b9ad-322f1ef01c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch ID: 2 - Error: An error occurred while calling o946.save.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy38.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "windowed_query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the Parquet files\n",
        "parquet_path = \"gs://edit-data-eng-project-group1/datalake/stream/windowed_vehicles_3\"\n",
        "\n",
        "# Read the Parquet files into a DataFrame\n",
        "parquet_df = spark.read.parquet(parquet_path)\n",
        "\n",
        "# Show the first few rows\n",
        "parquet_df.show(truncate=False)\n",
        "\n",
        "# Print the schema to understand the data structure\n",
        "parquet_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EENieyesqCOV",
        "outputId": "0a26af4b-d628-4b8f-ac78-efbc5066ed29"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+--------+-------------------+------------+------------+---------+---------+------------+--------------------+\n",
            "|window                                    |id      |max_ts             |previous_lat|previous_lon|lat      |lon      |distance    |speed               |\n",
            "+------------------------------------------+--------+-------------------+------------+------------+---------+---------+------------+--------------------+\n",
            "|{2025-01-22 08:20:00, 2025-01-22 08:22:00}|44|12620|2025-01-22 08:21:54|38.622612   |-8.861655   |38.623722|-8.867623|0.53295755  |15.988726615905762  |\n",
            "|{2025-01-22 08:20:00, 2025-01-22 08:22:00}|42|2313 |2025-01-22 08:21:38|38.804256   |-9.120041   |38.801754|-9.122209|0.33572677  |10.071803033351898  |\n",
            "|{2025-01-22 08:20:00, 2025-01-22 08:22:00}|43|669  |2025-01-22 08:20:48|38.565853   |-9.039958   |38.565853|-9.039958|0.0         |0.0                 |\n",
            "|{2025-01-22 08:20:00, 2025-01-22 08:22:00}|41|1390 |2025-01-22 08:21:26|38.7773     |-9.335617   |38.776752|-9.334625|0.10546574  |3.1639721989631653  |\n",
            "|{2025-01-22 08:20:00, 2025-01-22 08:22:00}|42|255  |2025-01-22 08:21:15|38.79201    |-9.172441   |38.79195 |-9.172944|0.04416547  |1.324964091181755   |\n",
            "|{2025-01-22 08:20:00, 2025-01-22 08:22:00}|43|2267 |2025-01-22 08:21:58|38.66486    |-9.177156   |38.664036|-9.179647|0.23488258  |7.046477347612381   |\n",
            "|{2025-01-22 18:30:00, 2025-01-22 18:32:00}|41|1338 |2025-01-22 18:31:36|38.73474    |-9.223644   |38.734577|-9.225052|0.123449065 |3.7034719437360764  |\n",
            "|{2025-01-22 18:30:00, 2025-01-22 18:32:00}|44|12089|2025-01-22 18:31:43|38.53294    |-8.890045   |38.53161 |-8.889495|0.15558262  |4.667478650808334   |\n",
            "|{2025-01-22 18:30:00, 2025-01-22 18:32:00}|41|543  |2025-01-22 18:31:49|38.84524    |-9.456219   |38.84101 |-9.460844|0.61785936  |18.535780906677246  |\n",
            "|{2025-01-22 18:30:00, 2025-01-22 18:32:00}|41|733  |2025-01-22 18:31:54|38.703033   |-9.342324   |38.703556|-9.342888|0.075954385 |2.278631553053856   |\n",
            "|{2025-01-22 18:30:00, 2025-01-22 18:32:00}|44|12657|2025-01-22 18:31:37|38.76776    |-9.100422   |38.76778 |-9.100397|0.0030198307|0.09059492032974958 |\n",
            "|{2025-01-22 18:30:00, 2025-01-22 18:32:00}|41|1321 |2025-01-22 18:30:50|38.7319     |-9.278984   |38.731895|-9.278992|7.8605395E-4|0.023581618443131447|\n",
            "|{2025-01-22 18:28:00, 2025-01-22 18:30:00}|43|2241 |2025-01-22 18:29:47|38.63774    |-9.202821   |38.63774 |-9.202821|0.0         |0.0                 |\n",
            "|{2025-01-22 08:30:00, 2025-01-22 08:32:00}|44|12694|2025-01-22 08:31:54|38.669388   |-8.97824    |38.666187|-8.98259 |0.51890504  |15.567151308059692  |\n",
            "|{2025-01-22 08:30:00, 2025-01-22 08:32:00}|42|1083 |2025-01-22 08:31:13|38.95049    |-9.331476   |38.950775|-9.332081|0.06120317  |1.836095117032528   |\n",
            "|{2025-01-22 08:30:00, 2025-01-22 08:32:00}|42|2502 |2025-01-22 08:31:59|38.794277   |-9.185575   |38.79304 |-9.183337|0.23766553  |7.1299660205841064  |\n",
            "|{2025-01-22 08:30:00, 2025-01-22 08:32:00}|44|12739|2025-01-22 08:31:42|38.530285   |-8.875915   |38.532295|-8.875533|0.22598952  |6.779685616493225   |\n",
            "|{2025-01-22 08:30:00, 2025-01-22 08:32:00}|41|334  |2025-01-22 08:31:52|38.764168   |-9.211755   |38.76525 |-9.213076|0.16621211  |4.986363351345062   |\n",
            "|{2025-01-22 18:36:00, 2025-01-22 18:38:00}|44|12696|2025-01-22 18:37:49|38.768948   |-9.104582   |38.768593|-9.102551|0.1803921   |5.411763042211533   |\n",
            "|{2025-01-22 18:36:00, 2025-01-22 18:38:00}|41|1802 |2025-01-22 18:37:52|38.757965   |-9.277612   |38.757885|-9.277627|0.009005399 |0.2701619826257229  |\n",
            "+------------------------------------------+--------+-------------------+------------+------------+---------+---------+------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- window: struct (nullable = true)\n",
            " |    |-- start: timestamp (nullable = true)\n",
            " |    |-- end: timestamp (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- max_ts: timestamp (nullable = true)\n",
            " |-- previous_lat: float (nullable = true)\n",
            " |-- previous_lon: float (nullable = true)\n",
            " |-- lat: float (nullable = true)\n",
            " |-- lon: float (nullable = true)\n",
            " |-- distance: float (nullable = true)\n",
            " |-- speed: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SceEgnFX-F2L",
        "outputId": "6c8da938-4087-4cef-c075-8a02d4281139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+\n",
            "|        min(max_ts)|        max(max_ts)|\n",
            "+-------------------+-------------------+\n",
            "|2025-01-19 14:25:29|2025-01-21 17:22:22|\n",
            "+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import min, max, col\n",
        "parquet_df.agg(min(col('max_ts')), max(col('max_ts'))).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the Parquet files\n",
        "parquet_path = \"gs://edit-data-eng-project-group1/datalake/stream/ETL\"\n",
        "\n",
        "# Read the Parquet files into a DataFrame\n",
        "parquet_df = spark.read.parquet(parquet_path)\n",
        "\n",
        "# Show the first few rows\n",
        "parquet_df.show(truncate=False)\n",
        "\n",
        "# Print the schema to understand the data structure\n",
        "parquet_df.printSchema()"
      ],
      "metadata": {
        "id": "eWcKKGFSrGsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}