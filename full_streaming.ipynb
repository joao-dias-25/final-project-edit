{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaniamv/final-project-edit/blob/main/full_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplication in Real Time to Read Carris API - group 1 - ETL approach\n",
        "\n",
        "This notebook documents the steps to implement a data pipeline leveraging Google Cloud Platform (GCP), following an ETL (Extract, Transform, Load) approach. The pipeline processes data in three stages:\n",
        "\n",
        "Streaming Ingestion and Transformation (Extract and Transform):\n",
        "Data is ingested in real-time from a bucket that gets the vehicles endpoint of  Carris API. During ingestion, transformations are applied directly to the data stream, such as cleaning, enrichment, and standardization, ensuring that only processed and structured data flows through the pipeline.\n",
        "\n",
        "Loading Transformed Data:\n",
        "The pre-processed data is then stored in a silver layer bucket on GCP. This layer serves as a structured repository, optimized for downstream analytical queries and consumption.\n",
        "\n",
        "By prioritizing the ETL approach, this pipeline ensures that the data is transformed as it is ingested, minimizing the need for post-processing and enabling faster delivery of structured and actionable insights.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fh0TmAWvVoCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Authentication to Google Cloud Platform (GCP)\n",
        "\n"
      ],
      "metadata": {
        "id": "B9KbgW3BVxtL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0-WlCSjaVZiz",
        "outputId": "384376c9-a76f-448c-8be4-b0ce08b43b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=bkX1TEEMxAk1ufk0TNMXLpxTBzMkvh&prompt=consent&token_usage=remote&access_type=offline&code_challenge=Th6LJRA5MLhPhEsNsIjFgdVCpHXQVdLv9MzdbZcQTCU&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0ASVgi3JmNVNq5TW3c2Urfe3hULxlpQj8BuzpruL0OpAeiyGFxDPccJhNjLv--IhlCKhZxA\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ],
      "source": [
        "# autentication to gcloud with login\n",
        "\n",
        "!gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download connector and save it local\n",
        "\n",
        "!wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar -P /usr/local/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IuHDKJ_SV3Qq",
        "outputId": "11098b2f-7bac-4baa-8300-12fa76746665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-22 13:58:52--  https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.7/gcs-connector-hadoop3-2.2.7-shaded.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33831577 (32M) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar’\n",
            "\n",
            "gcs-connector-hadoo 100%[===================>]  32.26M   172MB/s    in 0.2s    \n",
            "\n",
            "2025-01-22 13:58:52 (172 MB/s) - ‘/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar’ saved [33831577/33831577]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "2. Initialize SparkSession and set up the access to GSC"
      ],
      "metadata": {
        "id": "y2eZbvF3iUc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('GCS_Spark') \\\n",
        "    .config('spark.jars', '/usr/local/lib/gcs-connector-hadoop3-2.2.7-shaded.jar') \\\n",
        "    .config('spark.hadoop.fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# save credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/content/.config/application_default_credentials.json'\n",
        "\n",
        "# Config PySpark to access the GCS\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", '/content/.config/application_default_credentials.json')"
      ],
      "metadata": {
        "id": "2XXRK45iV5s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "3. Set up the source schema and initialize the readStream"
      ],
      "metadata": {
        "id": "4bbr84t-WILm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# create schema\n",
        "vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                             StructField('block_id', StringType(), True),\n",
        "                             StructField('current_status', StringType(), True),\n",
        "                             StructField('id', StringType(), True),\n",
        "                             StructField('lat', FloatType(), True),\n",
        "                             StructField('line_id', StringType(), True),\n",
        "                             StructField('lon', FloatType(), True),\n",
        "                             StructField('pattern_id', StringType(), True),\n",
        "                             StructField('route_id', StringType(), True),\n",
        "                             StructField('schedule_relationship', StringType(), True),\n",
        "                             StructField('shift_id', StringType(), True),\n",
        "                             StructField('speed', FloatType(), True),\n",
        "                             StructField('stop_id', StringType(), True),\n",
        "                             StructField('timestamp', TimestampType(), True),\n",
        "                             StructField('trip_id', StringType(), True)])\n",
        "\n",
        "\n",
        "#readStreaming\n",
        "stream = spark.readStream.format(\"json\").schema(vehicle_schema).load(\"gs://edit-de-project-streaming-data/carris-vehicles\")"
      ],
      "metadata": {
        "id": "d6nbxv9sWL-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "4. Write the stream in a bronze layer (landing zone)\n",
        "* Purpose: Raw data ingestion layer.\n",
        "* Data Characteristics: Raw, unprocessed, and schema-on-read where feasible.\n",
        "* Data Storage: Store data exactly as ingested (in this case JSON format).\n",
        "* Operations: Minimal transformation; only schema enforcement and deduplication."
      ],
      "metadata": {
        "id": "4lMfv4ejWVY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_stops = spark.read.option(\"header\", \"true\").csv('gs://edit-data-eng-project-group1/LandingZone/GTFS/stops.txt')\n",
        "df_stops = df_stops.select('stop_id','stop_lat','stop_lon')\n",
        "df_stops = df_stops.withColumn(\"stop_lat\", df_stops[\"stop_lat\"].cast(\"float\"))\n",
        "df_stops = df_stops.withColumn(\"stop_lon\", df_stops[\"stop_lon\"].cast(\"float\"))"
      ],
      "metadata": {
        "id": "vELIsRG_WRTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select columns\n",
        "transform = stream.select('id', 'speed', 'timestamp','line_id','route_id','stop_id','lat', 'lon')\n",
        "# join tables\n",
        "transform = transform.join(df_stops, on='stop_id', how='left')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fIS5epvuhXMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col, lag,coalesce, current_timestamp, window\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# watermark is necessarary because of the aggregation\n",
        "transformed = transform.withWatermark(\"timestamp\", \"60 seconds\")\n",
        "\n",
        "windowed_transform = transformed.groupBy(\"id\", \"stop_id\", F.window(\"timestamp\", \"2 minutes\")).agg(\n",
        "    F.first(col(\"lat\")).alias(\"previous_lat\"),\n",
        "    F.first(col(\"lon\")).alias(\"previous_lon\"),\n",
        "    F.last(col(\"lat\")).alias(\"lat\"),\n",
        "    F.last(col(\"lon\")).alias(\"lon\"),\n",
        "    F.last(col(\"stop_lat\")).alias(\"stop_lat\"),\n",
        "    F.last(col(\"stop_lon\")).alias(\"stop_lon\")\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G7jkm0qSpW7E",
        "outputId": "16048966-5d8b-4491-d593-999111235046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[MISSING_AGGREGATION] The non-aggregating expression \"lat\" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use \"any_value(lat)\" if you do not care which of the values within a group is returned.;\nProject [id#12376, stop_id#12385, window#12611-T60000ms, previous_lat#12623, previous_lon#12625, lat#12627, lon#12629, stop_lat#12631, stop_lon#12633]\n+- Project [id#12376, stop_id#12385, window#12611-T60000ms, lat#12377, timestamp#12386-T60000ms, lon#12379, stop_lat#12584, stop_lon#12588, previous_lat#12623, previous_lon#12625, lat#12627, lon#12629, stop_lat#12631, stop_lon#12633, previous_lat#12623, previous_lon#12625, lat#12627, lon#12629, stop_lat#12631, stop_lon#12633]\n   +- Window [first(lat#12377, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS previous_lat#12623, first(lon#12379, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS previous_lon#12625, last(lat#12377, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS lat#12627, last(lon#12379, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS lon#12629, last(stop_lat#12584, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS stop_lat#12631, last(stop_lon#12588, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS stop_lon#12633], [id#12376], [timestamp#12386-T60000ms ASC NULLS FIRST]\n      +- Aggregate [id#12376, stop_id#12385, window#12634-T60000ms], [id#12376, stop_id#12385, window#12634-T60000ms AS window#12611-T60000ms, lat#12377, timestamp#12386-T60000ms, lon#12379, stop_lat#12584, stop_lon#12588]\n         +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) + 120000000) ELSE ((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) + 120000000) ELSE ((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) END) - 0) + 120000000), LongType, TimestampType))) AS window#12634-T60000ms, stop_id#12385, id#12376, speed#12384, timestamp#12386-T60000ms, line_id#12378, route_id#12381, lat#12377, lon#12379, stop_lat#12584, stop_lon#12588]\n            +- Filter isnotnull(timestamp#12386-T60000ms)\n               +- EventTimeWatermark timestamp#12386: timestamp, 1 minutes\n                  +- Project [stop_id#12385, id#12376, speed#12384, timestamp#12386, line_id#12378, route_id#12381, lat#12377, lon#12379, stop_lat#12584, stop_lon#12588]\n                     +- Join LeftOuter, (stop_id#12385 = stop_id#12420)\n                        :- Project [id#12376, speed#12384, timestamp#12386, line_id#12378, route_id#12381, stop_id#12385, lat#12377, lon#12379]\n                        :  +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@2e3288b6,json,List(),Some(StructType(StructField(bearing,IntegerType,true),StructField(block_id,StringType,true),StructField(current_status,StringType,true),StructField(id,StringType,true),StructField(lat,FloatType,true),StructField(line_id,StringType,true),StructField(lon,FloatType,true),StructField(pattern_id,StringType,true),StructField(route_id,StringType,true),StructField(schedule_relationship,StringType,true),StructField(shift_id,StringType,true),StructField(speed,FloatType,true),StructField(stop_id,StringType,true),StructField(timestamp,TimestampType,true),StructField(trip_id,StringType,true))),List(),None,Map(path -> gs://edit-de-project-streaming-data/carris-vehicles),None), FileSource[gs://edit-de-project-streaming-data/carris-vehicles], [bearing#12373, block_id#12374, current_status#12375, id#12376, lat#12377, line_id#12378, lon#12379, pattern_id#12380, route_id#12381, schedule_relationship#12382, shift_id#12383, speed#12384, stop_id#12385, timestamp#12386, trip_id#12387]\n                        +- Project [stop_id#12420, stop_lat#12584, cast(stop_lon#12425 as float) AS stop_lon#12588]\n                           +- Project [stop_id#12420, cast(stop_lat#12424 as float) AS stop_lat#12584, stop_lon#12425]\n                              +- Project [stop_id#12420, stop_lat#12424, stop_lon#12425]\n                                 +- Relation [stop_id#12420,stop_name#12421,stop_name_new#12422,stop_short_name#12423,stop_lat#12424,stop_lon#12425,operational_status#12426,areas#12427,region_id#12428,region_name#12429,district_id#12430,district_name#12431,municipality_id#12432,municipality_name#12433,parish_id#12434,parish_name#12435,locality#12436,jurisdiction#12437,stop_code#12438,tts_stop_name#12439,platform_code#12440,parent_station#12441,location_type#12442,stop_url#12443,... 56 more fields] csv\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-9c0e09aa5640>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithWatermark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"60 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m windowed_transform = transformed.groupBy(\"id\", \"stop_id\", F.window(\"timestamp\", \"2 minutes\")).agg(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"previous_lat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lon\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"previous_lon\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mexprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [MISSING_AGGREGATION] The non-aggregating expression \"lat\" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use \"any_value(lat)\" if you do not care which of the values within a group is returned.;\nProject [id#12376, stop_id#12385, window#12611-T60000ms, previous_lat#12623, previous_lon#12625, lat#12627, lon#12629, stop_lat#12631, stop_lon#12633]\n+- Project [id#12376, stop_id#12385, window#12611-T60000ms, lat#12377, timestamp#12386-T60000ms, lon#12379, stop_lat#12584, stop_lon#12588, previous_lat#12623, previous_lon#12625, lat#12627, lon#12629, stop_lat#12631, stop_lon#12633, previous_lat#12623, previous_lon#12625, lat#12627, lon#12629, stop_lat#12631, stop_lon#12633]\n   +- Window [first(lat#12377, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS previous_lat#12623, first(lon#12379, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS previous_lon#12625, last(lat#12377, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS lat#12627, last(lon#12379, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS lon#12629, last(stop_lat#12584, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS stop_lat#12631, last(stop_lon#12588, false) windowspecdefinition(id#12376, timestamp#12386-T60000ms ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS stop_lon#12633], [id#12376], [timestamp#12386-T60000ms ASC NULLS FIRST]\n      +- Aggregate [id#12376, stop_id#12385, window#12634-T60000ms], [id#12376, stop_id#12385, window#12634-T60000ms AS window#12611-T60000ms, lat#12377, timestamp#12386-T60000ms, lon#12379, stop_lat#12584, stop_lon#12588]\n         +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) + 120000000) ELSE ((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) + 120000000) ELSE ((precisetimestampconversion(timestamp#12386-T60000ms, TimestampType, LongType) - 0) % 120000000) END) - 0) + 120000000), LongType, TimestampType))) AS window#12634-T60000ms, stop_id#12385, id#12376, speed#12384, timestamp#12386-T60000ms, line_id#12378, route_id#12381, lat#12377, lon#12379, stop_lat#12584, stop_lon#12588]\n            +- Filter isnotnull(timestamp#12386-T60000ms)\n               +- EventTimeWatermark timestamp#12386: timestamp, 1 minutes\n                  +- Project [stop_id#12385, id#12376, speed#12384, timestamp#12386, line_id#12378, route_id#12381, lat#12377, lon#12379, stop_lat#12584, stop_lon#12588]\n                     +- Join LeftOuter, (stop_id#12385 = stop_id#12420)\n                        :- Project [id#12376, speed#12384, timestamp#12386, line_id#12378, route_id#12381, stop_id#12385, lat#12377, lon#12379]\n                        :  +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@2e3288b6,json,List(),Some(StructType(StructField(bearing,IntegerType,true),StructField(block_id,StringType,true),StructField(current_status,StringType,true),StructField(id,StringType,true),StructField(lat,FloatType,true),StructField(line_id,StringType,true),StructField(lon,FloatType,true),StructField(pattern_id,StringType,true),StructField(route_id,StringType,true),StructField(schedule_relationship,StringType,true),StructField(shift_id,StringType,true),StructField(speed,FloatType,true),StructField(stop_id,StringType,true),StructField(timestamp,TimestampType,true),StructField(trip_id,StringType,true))),List(),None,Map(path -> gs://edit-de-project-streaming-data/carris-vehicles),None), FileSource[gs://edit-de-project-streaming-data/carris-vehicles], [bearing#12373, block_id#12374, current_status#12375, id#12376, lat#12377, line_id#12378, lon#12379, pattern_id#12380, route_id#12381, schedule_relationship#12382, shift_id#12383, speed#12384, stop_id#12385, timestamp#12386, trip_id#12387]\n                        +- Project [stop_id#12420, stop_lat#12584, cast(stop_lon#12425 as float) AS stop_lon#12588]\n                           +- Project [stop_id#12420, cast(stop_lat#12424 as float) AS stop_lat#12584, stop_lon#12425]\n                              +- Project [stop_id#12420, stop_lat#12424, stop_lon#12425]\n                                 +- Relation [stop_id#12420,stop_name#12421,stop_name_new#12422,stop_short_name#12423,stop_lat#12424,stop_lon#12425,operational_status#12426,areas#12427,region_id#12428,region_name#12429,district_id#12430,district_name#12431,municipality_id#12432,municipality_name#12433,parish_id#12434,parish_name#12435,locality#12436,jurisdiction#12437,stop_code#12438,tts_stop_name#12439,platform_code#12440,parent_station#12441,location_type#12442,stop_url#12443,... 56 more fields] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sqrt, pow\n",
        "\n",
        "windowed_transform = windowed_transform.withColumn(\"distance\",\n",
        "      sqrt(\n",
        "        pow(col(\"lat\") - col(\"previous_lat\"), 2) + pow(col(\"lon\") - col(\"previous_lon\"), 2)\n",
        "    )* 110 # each degree is ~100km\n",
        "                                                   )\n",
        "\n",
        "windowed_transform = windowed_transform.withColumn(\"distance_to_stop\",      sqrt(\n",
        "        pow(col(\"lat\") - col(\"stop_lat\"), 2) + pow(col(\"lon\") - col(\"stop_lon\"), 2)\n",
        "    )* 110 # each degree is ~110km\n",
        "                                                   )"
      ],
      "metadata": {
        "id": "GbBvKd5PeCtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg = windowed_transform.withColumn('speed', col('distance')/(2/60))\n",
        "\n",
        "agg = agg.withColumn('time_to_stop', (col('distance_to_stop')/col('speed') * 3600))\n",
        "\n",
        "agg = agg.withColumn(\n",
        "    'time_to_stop',\n",
        "    F.from_unixtime(\n",
        "        F.unix_timestamp(F.lit('00:00:00'), 'HH:mm:ss') + col('time_to_stop'),\n",
        "        'HH:mm:ss'\n",
        "    ))"
      ],
      "metadata": {
        "id": "SGF0LtzIgx7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write Stream"
      ],
      "metadata": {
        "id": "_J96nYkbXB06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select folder\n",
        "folder = 'stream/vehicles6'\n",
        "gc_folder = 'gs://edit-de-project-streaming-data/datalake/stream/vehicles'\n",
        "\n",
        "\n",
        "# Output function for each windowed batch\n",
        "def insert_windowed_vehicles(df, batch_id):\n",
        "    print(f\"Batch ID: {batch_id}\")\n",
        "    df.write.format(\"parquet\").mode(\"append\").save(f\"{folder}\")\n",
        "\n",
        "\n",
        "# Write the streaming query with watermark and window\n",
        "windowed_query = (agg\n",
        "                  .writeStream\n",
        "                  .outputMode(\"append\")\n",
        "                  .foreachBatch(insert_windowed_vehicles)\n",
        "                  .option('checkpointLocation', f'{folder}/checkpoint')\n",
        "                  .trigger(processingTime='10 seconds')\n",
        "                  .start()\n",
        ")\n",
        "\n",
        "windowed_query.awaitTermination(30)"
      ],
      "metadata": {
        "id": "eaDegbFrhpqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c324ec87-7f81-4bc5-f577-cb3ef029e86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.isActive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SPUcGoQiipf",
        "outputId": "0cde3cc6-3419-4b80-b620-fb19ce6170e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C04cg6jJxTfM",
        "outputId": "1f7d8ae9-8988-4938-e8f2-8b922d9b0899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.recentProgress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n90r0H5bhS7K",
        "outputId": "cf4ae8c3-1824-49cc-9395-ca3da029dffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.stop()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xKTj7O-Qiqxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RkylzaJJ_dx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Transformations"
      ],
      "metadata": {
        "id": "bmsfBvacWi5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WdiewdrydX7p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMgMV1oVWiCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_query.recentProgress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve7gJIYxw2hY",
        "outputId": "6e350d5a-fee3-4be9-fd3c-0841c5d045c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the Parquet files\n",
        "parquet_path = \"stream/vehicles5\"\n",
        "\n",
        "# Read the Parquet files into a DataFrame\n",
        "parquet_df = spark.read.parquet(parquet_path)\n",
        "\n",
        "# Show the first few rows\n",
        "parquet_df.show(truncate=False)\n",
        "\n",
        "# Print the schema to understand the data structure\n",
        "parquet_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MV5AxogihP9",
        "outputId": "866c9916-7a61-4313-96df-94c53d2c9d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+------------------------------------------+------------+------------+---------+---------+---------+---------+--------------------+--------------------+------------------+------------+\n",
            "|id      |stop_id|window                                    |previous_lat|previous_lon|lat      |lon      |stop_lat |stop_lon |distance            |distance_to_stop    |speed             |time_to_stop|\n",
            "+--------+-------+------------------------------------------+------------+------------+---------+---------+---------+---------+--------------------+--------------------+------------------+------------+\n",
            "|43|2339 |140349 |{2025-01-22 13:18:00, 2025-01-22 13:20:00}|38.637917   |-9.154437   |38.63827 |-9.154015|38.63906 |-9.152929|0.060684412994725434|0.14763666595197703 |1.8205323898417631|00:04:51    |\n",
            "|41|1423 |171629 |{2025-01-21 13:42:00, 2025-01-21 13:44:00}|38.904125   |-9.416783   |38.904636|-9.414741|38.9048  |-9.413896|0.2316330739949299  |0.09468030391511913 |6.948992219847898 |00:00:49    |\n",
            "|41|1341 |030726 |{2025-01-21 20:00:00, 2025-01-21 20:02:00}|38.751858   |-9.224149   |38.751858|-9.224149|38.75594 |-9.225609|0.0                 |0.4768510512235419  |0.0               |NULL        |\n",
            "|43|2613 |140445 |{2025-01-21 16:56:00, 2025-01-21 16:58:00}|38.568245   |-9.08545    |38.567844|-9.082514|38.568283|-9.088538|0.32599115371390236 |0.6644343322378443  |9.779734611417071 |00:04:04    |\n",
            "|42|2532 |110509 |{2025-01-21 09:34:00, 2025-01-21 09:36:00}|38.80678    |-9.196329   |38.80678 |-9.196329|38.806515|-9.195622|0.0                 |0.08309847654472652 |0.0               |NULL        |\n",
            "|44|12730|160103 |{2025-01-21 23:06:00, 2025-01-21 23:08:00}|38.52332    |-8.888797   |38.52332 |-8.888797|38.5232  |-8.890126|0.0                 |0.1468138330186547  |0.0               |NULL        |\n",
            "|42|1203 |070070 |{2025-01-20 19:46:00, 2025-01-20 19:48:00}|38.833225   |-9.196336   |38.833225|-9.196336|38.832954|-9.195743|0.0                 |0.07173021922237287 |0.0               |NULL        |\n",
            "|44|12090|100051 |{2025-01-21 14:26:00, 2025-01-21 14:28:00}|38.710617   |-8.953815   |38.710617|-8.953815|38.71005 |-8.953756|0.0                 |0.06286027623376238 |0.0               |NULL        |\n",
            "|41|1279 |170896 |{2025-01-21 10:48:00, 2025-01-21 10:50:00}|38.76542    |-9.259912   |38.764744|-9.259628|38.764744|-9.259797|0.08054243043944066 |0.018568038940429688|2.41627291318322  |00:00:27    |\n",
            "|41|1872 |171111 |{2025-01-21 13:46:00, 2025-01-21 13:48:00}|38.82463    |-9.294533   |38.82463 |-9.294533|38.825043|-9.29498 |0.0                 |0.06689111697509759 |0.0               |NULL        |\n",
            "|41|1125 |030763 |{2025-01-21 16:22:00, 2025-01-21 16:24:00}|38.73876    |-9.211807   |38.73876 |-9.211807|38.73786 |-9.210943|0.0                 |0.13725908534359976 |0.0               |NULL        |\n",
            "|41|1263 |170781 |{2025-01-21 15:10:00, 2025-01-21 15:12:00}|38.757137   |-9.306819   |38.754578|-9.306728|38.75777 |-9.306862|0.2817391210918805  |0.3515261103088866  |8.452173632756415 |00:02:29    |\n",
            "|42|213  |110297 |{2025-01-21 17:50:00, 2025-01-21 17:52:00}|38.787354   |-9.183604   |38.787354|-9.183604|38.78717 |-9.184461|0.0                 |0.09633311017460876 |0.0               |NULL        |\n",
            "|41|1244 |172235 |{2025-01-22 07:38:00, 2025-01-22 07:40:00}|38.751675   |-9.351923   |38.751675|-9.351923|38.750862|-9.351385|0.0                 |0.1071872234488623  |0.0               |NULL        |\n",
            "|42|1210 |070210 |{2025-01-22 13:08:00, 2025-01-22 13:10:00}|38.90348    |-9.19035    |38.90348 |-9.19035 |38.902813|-9.193353|0.0                 |0.3384066408457703  |0.0               |NULL        |\n",
            "|44|12064|160629 |{2025-01-22 07:56:00, 2025-01-22 07:58:00}|38.53443    |-8.89661    |38.534454|-8.89657 |38.53439 |-8.896629|0.005074586976887743|0.009653464338195857|0.1522376093066323|00:03:48    |\n",
            "|43|2113 |140794 |{2025-01-20 19:12:00, 2025-01-20 19:14:00}|38.564823   |-9.077907   |38.563835|-9.079389|38.563854|-9.079367|0.19592696349710284 |0.003197426933201025|5.877808904913086 |00:00:01    |\n",
            "|42|2777 |180041 |{2025-01-22 12:56:00, 2025-01-22 12:58:00}|38.893356   |-9.035484   |38.893356|-9.035484|38.89053 |-9.034227|0.0                 |0.3402910980351042  |0.0               |NULL        |\n",
            "|42|2375 |071463 |{2025-01-20 21:20:00, 2025-01-20 21:22:00}|38.810223   |-9.099498   |38.810432|-9.099302|38.81064 |-9.102295|0.031545472048054475|0.3299682402054218  |0.9463641614416343|00:20:55    |\n",
            "|42|1270 |080370 |{2025-01-20 18:40:00, 2025-01-20 18:42:00}|38.93799    |-9.200997   |38.93799 |-9.200997|38.934216|-9.202238|0.0                 |0.4368668455642884  |0.0               |NULL        |\n",
            "+--------+-------+------------------------------------------+------------+------------+---------+---------+---------+---------+--------------------+--------------------+------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- stop_id: string (nullable = true)\n",
            " |-- window: struct (nullable = true)\n",
            " |    |-- start: timestamp (nullable = true)\n",
            " |    |-- end: timestamp (nullable = true)\n",
            " |-- previous_lat: float (nullable = true)\n",
            " |-- previous_lon: float (nullable = true)\n",
            " |-- lat: float (nullable = true)\n",
            " |-- lon: float (nullable = true)\n",
            " |-- stop_lat: float (nullable = true)\n",
            " |-- stop_lon: float (nullable = true)\n",
            " |-- distance: double (nullable = true)\n",
            " |-- distance_to_stop: double (nullable = true)\n",
            " |-- speed: double (nullable = true)\n",
            " |-- time_to_stop: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GoIrIPqBicIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}